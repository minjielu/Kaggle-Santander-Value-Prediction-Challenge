{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to clean outliers using the first round of prediction.\n",
    "First, ese a machine learning model to predict target. Then, remove rows where predictions are far away from true target. Afterwards, use the cleaned train set to train a model again. This may prevent the model to overfit train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load libraries for analysis and visualization\n",
    "import pandas as pd # collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "import numpy as np  # foundational package for scientific computing\n",
    "import re           # Regular expression operations\n",
    "import matplotlib.pyplot as plt # Collection of functions for scientific and publication-ready visualization\n",
    "import lightgbm as lgb\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py     # Open source library for composing, editing, and sharing interactive data visualization \n",
    "from matplotlib import pyplot\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from collections import Counter\n",
    "\n",
    "# Machine learning libraries\n",
    "import sys\n",
    "sys.path.append('/Users/minjielu/anaconda3/envs/python/lib/python3.5/site-packages')\n",
    "\n",
    "import xgboost as xgb  # Implementation of gradient boosted decision trees designed for speed and performance that is dominative competitive machine learning\n",
    "import seaborn as sns  # Visualization library based on matplotlib, provides interface for drawing attractive statistical graphics\n",
    "\n",
    "import sklearn         # Collection of machine learning algorithms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report, precision_recall_curve, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "train = pd.read_csv('./train.csv')\n",
    "#test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations of each column to target. Zeros are removed\n",
    "nonzero_corr = []\n",
    "for column in train.columns:\n",
    "    if column != 'ID' and column != 'target':\n",
    "        train_1 = train[train[column] != 0][['target',column]]\n",
    "        corr_tmp = train_1.corr()\n",
    "        nonzero_corr.append([corr_tmp.loc['target',column],train_1.shape[0]])\n",
    "        \n",
    "column_label = list(train.columns)\n",
    "column_label.pop(0) # Remove column 'ID'\n",
    "column_label.pop(0) # Remove column 'target'\n",
    "\n",
    "nonzero_corr = pd.DataFrame(nonzero_corr,columns=['correlation_to_target','nonzero_cnt'],index=column_label)\n",
    "# nonzero_corr = nonzero_corr.sort_values(['target'])\n",
    "# pd.set_option('display.max_rows',5000)\n",
    "# nonzero_corr\n",
    "# nonzero_corr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and targets for machine learning models\n",
    "train_y = train['target']\n",
    "train_x = train.loc[:,(train.columns !='ID') & (train.columns != 'target')].loc[:,((nonzero_corr['correlation_to_target'] > 0.02) | (nonzero_corr['correlation_to_target'] < -0.1))] # Abandon features with small absolute correlations to target\n",
    "#test_x = test.loc[:,(test.columns != 'ID') & (test.columns != 'target')].loc[:,((nonzero_corr['target'] > 0.4) | (nonzero_corr['target'] < -0.1)) & (nonzero_corr['nonzero_cnt'] > 20)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def fit_predict(data, y, test):\n",
    "    # Create folds\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    # Init predictions\n",
    "    # sub_preds = np.zeros(test.shape[0])\n",
    "    oof_preds = np.zeros(data.shape[0])\n",
    "    # Convert to lightgbm Dataset\n",
    "    dtrain = lgb.Dataset(data=data, label=np.log1p(y), free_raw_data=False)\n",
    "    # Construct dataset so that we can use slice()\n",
    "    dtrain.construct()\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': 58,\n",
    "        'subsample': 0.6143,\n",
    "        'colsample_bytree': 0.6453,\n",
    "        'min_split_gain': np.power(10, -2.5988),\n",
    "        'reg_alpha': np.power(10, -2.2887),\n",
    "        'reg_lambda': np.power(10, 1.7570),\n",
    "        'min_child_weight': np.power(10, -0.1477),\n",
    "        'verbose': -1,\n",
    "        'seed': 3,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': -1,\n",
    "        'learning_rate': 0.05,\n",
    "        'metric': 'l2',\n",
    "    }\n",
    "    # Run KFold\n",
    "    for trn_idx, val_idx in folds.split(data):\n",
    "        # Train lightgbm\n",
    "        clf = lgb.train(\n",
    "            params=lgb_params,\n",
    "            train_set=dtrain.subset(trn_idx),\n",
    "            valid_sets=dtrain.subset(val_idx),\n",
    "            num_boost_round=10000, \n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=50\n",
    "        )\n",
    "        # Predict Out Of Fold and Test targets\n",
    "        # Using lgb.train, predict will automatically select the best round for prediction\n",
    "        oof_preds[val_idx] = clf.predict(dtrain.data.iloc[val_idx])\n",
    "        # sub_preds += clf.predict(test.loc[:,(test.columns != 'ID')]) / folds.n_splits\n",
    "        # Display current fold score\n",
    "        print(mean_squared_error(np.log1p(y.iloc[val_idx]), \n",
    "                                 oof_preds[val_idx]) ** .5)\n",
    "    # Display Full OOF score (square root of a sum is not the sum of square roots)\n",
    "    print('Full Out-Of-Fold score : %9.6f' \n",
    "          % (mean_squared_error(np.log1p(y), oof_preds) ** .5))\n",
    "\n",
    "    return oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l2: 2.03718\n",
      "[100]\tvalid_0's l2: 1.94138\n",
      "[150]\tvalid_0's l2: 1.94115\n",
      "[200]\tvalid_0's l2: 1.95948\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's l2: 1.93645\n",
      "1.3915637185732705\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l2: 2.28524\n",
      "[100]\tvalid_0's l2: 2.15999\n",
      "[150]\tvalid_0's l2: 2.14516\n",
      "[200]\tvalid_0's l2: 2.14764\n",
      "Early stopping, best iteration is:\n",
      "[142]\tvalid_0's l2: 2.14237\n",
      "1.4636849050173035\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l2: 2.26976\n",
      "[100]\tvalid_0's l2: 2.14192\n",
      "[150]\tvalid_0's l2: 2.11737\n",
      "[200]\tvalid_0's l2: 2.11969\n",
      "[250]\tvalid_0's l2: 2.11444\n",
      "[300]\tvalid_0's l2: 2.12154\n",
      "Early stopping, best iteration is:\n",
      "[247]\tvalid_0's l2: 2.11362\n",
      "1.453828070805005\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l2: 2.15221\n",
      "[100]\tvalid_0's l2: 2.00902\n",
      "[150]\tvalid_0's l2: 1.96438\n",
      "[200]\tvalid_0's l2: 1.95089\n",
      "[250]\tvalid_0's l2: 1.95246\n",
      "[300]\tvalid_0's l2: 1.95278\n",
      "Early stopping, best iteration is:\n",
      "[212]\tvalid_0's l2: 1.94657\n",
      "1.3951962067973827\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l2: 1.99035\n",
      "[100]\tvalid_0's l2: 1.914\n",
      "[150]\tvalid_0's l2: 1.9256\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid_0's l2: 1.9127\n",
      "1.3830039418347413\n",
      "Full Out-Of-Fold score :  1.417873\n"
     ]
    }
   ],
   "source": [
    "oof_preds = fit_predict(train_x,train_y,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofpreds = pd.Series(oof_preds,name='predicted_target')\n",
    "train_y = train_y.rename('target')\n",
    "train_y_tmp = pd.concat([oofpreds,train_y],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute log error between prediction and true target\n",
    "train_y_tmp['error'] = abs(train_y_tmp['predicted_target']-np.log1p(train_y_tmp['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows having error larger than 1.2 \n",
    "train_y = train_y_tmp.loc[train_y_tmp['error'] < 1.2,'target']\n",
    "train_x = train_x.loc[train_y_tmp['error'] < 1.2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2729"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.7,\n",
       "       colsample_bytree=0.7, gamma=0.002, grow_policy='lossguide',\n",
       "       learning_rate=0.02, max_delta_step=0, max_depth=8, max_leaves=31,\n",
       "       min_child_weight=5, missing=None, n_estimators=1000, n_jobs=-1,\n",
       "       nthread=None, objective='reg:linear', random_state=42,\n",
       "       reg_alpha=0.002, reg_lambda=1.0, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8, tree_method='hist')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Train an XGBooost regressor\n",
    "params = {\n",
    "    'max_depth': 8, \n",
    "    'learning_rate': 0.02, \n",
    "    'n_estimators': 1000, \n",
    "    'silent': True, \n",
    "    'objective': 'reg:linear', \n",
    "    'booster': 'gbtree', \n",
    "    'n_jobs': -1, \n",
    "    'gamma': 0.002,\n",
    "    'min_child_weight': 5, \n",
    "    'subsample': 0.8, \n",
    "    'colsample_bytree': 0.7, \n",
    "    'colsample_bylevel': 0.7, \n",
    "    'reg_alpha': 0.002, \n",
    "    'reg_lambda': 1.0, \n",
    "    'random_state': 42}\n",
    "\n",
    "params['tree_method'] = 'hist'\n",
    "params['grow_policy'] = 'lossguide'\n",
    "params['max_leaves']  = 31\n",
    "\n",
    "test_x = test.loc[:,(test.columns != 'ID') & (test.columns != 'target')].loc[:,((nonzero_corr['target'] >= 0.2) | (nonzero_corr['target'] <= -0.2)) & (nonzero_corr['nonzero_cnt'] > 20)]\n",
    "\n",
    "gbm = xgb.XGBRegressor(**params)\n",
    "gbm.fit(train_x, train_y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission file\n",
    "customerid = train['ID']\n",
    "# result = pd.Series(gbm.predict(test_x),name='target')\n",
    "result = pd.Series(oof_preds,name='target')\n",
    "result = pd.concat([customerid,result],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't allow test targets to be smaller than the minimum value of train targets\n",
    "# min_value = train_y.min()\n",
    "# result.loc[result['target'] < min_value,'target'] = min_value\n",
    "result.to_csv('Clean_outliers_using_prediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
